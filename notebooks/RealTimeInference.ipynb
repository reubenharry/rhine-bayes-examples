{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26006fb6",
   "metadata": {},
   "source": [
    "# Real-time inference with Functional Reactive Probabilistic Programming\n",
    "\n",
    "![text](https://monad-bayes-site.netlify.app/images/ezgif-3-f3ddcd7da9.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f76fa",
   "metadata": {},
   "source": [
    "In the above gif, the green dots represents a latent variable, the unseen but true position of two agents, which are moving stochastically around a 2D environment. \n",
    "What the system actually sees are the noisy observations shown in red.\n",
    "The purple swarm is a population of particles from a particle filter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff9ff3",
   "metadata": {},
   "source": [
    "## The approach\n",
    "\n",
    "In the simulation from which the above gif is made, the inference is performed **in real-time**, using a particle filter.\n",
    "\n",
    "It is implemented using a combination of (1) a paradigm for representing Bayesian probability and inference known as **probabilistic programming**, and (2) a paradigm for representing real-time interactive systems known as (functional) **reactive programming**.\n",
    "\n",
    "There are several appealing aspects to this approach:\n",
    "\n",
    "- it offers a natural representation for continuous time stochastic processes. Time is continuous in the sense that you write code that is agnostic to the final sampling rate of the whole system, and so can be arbitrarily fine-grained.\n",
    "\n",
    "\n",
    "- it decouples the specification of the prior and posterior from the inference algorithm. For example, in the code for the above gif, the model is described by a prior, which is a stochastic process, and the posterior is described by a function from one stochastic process to another, mapping from a stream of observations to a process representing the posterior. \n",
    "\n",
    "\n",
    "- inference methods can be designed compositionally in a similar manner to standard probabilistic programming languages. For example, we may want to add MH moves at various points, or to adaptively change the population size or resampling rate. These extensions fit naturally into the approach.\n",
    "\n",
    "\n",
    "- it allows for simulations to be run indefinitely long without concerns about time or space leaks.\n",
    "\n",
    "\n",
    "- it allows for observations to come from multiple streams, at different rates, or even from user input. For example, the frame rate of the simulation (say 60Hz) need not be the rate at which observations are received and processed. So if inference is slow at times, this can be handled gracefully.\n",
    "\n",
    "\n",
    "- it allows for action in the loop. For example, one might want to take actions in real time, based on the current belief state, which would in turn influence incoming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f715c",
   "metadata": {},
   "source": [
    "# What the code looks like\n",
    "\n",
    "The code to produce the above simulation is simple and declarative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b1326",
   "metadata": {},
   "source": [
    "## The prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d7d93",
   "metadata": {},
   "source": [
    "```haskell\n",
    "type Observation = V2 Double\n",
    "type Position = V2 Double\n",
    "\n",
    "prior :: StochasticProcess Position\n",
    "prior = fmap (uncurry V2) $ walk1D &&& walk1D where\n",
    "\n",
    "    walk1D = proc _ -> do\n",
    "        acceleration <- constM (normal 0 4 ) -< ()\n",
    "        velocity <- decayIntegral 2 -< acceleration -- Integral, dying off exponentially\n",
    "        position <- decayIntegral 2 -< velocity\n",
    "        returnA -< position\n",
    "\n",
    "    decayIntegral timeConstant =  average timeConstant >>> arr (timeConstant *^)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37146961",
   "metadata": {},
   "source": [
    "`prior` describes a stochastic process that serves as the prior about how a (single) dot moves.\n",
    "\n",
    "It is built by taking a constantly normally distributed stream of accelerations, and integrating it twice. That gives the circle's movement on one axis, so we join two of these together.\n",
    "\n",
    "Note how this is an abstract mathematical object: it is not an event loop for example - it is simply a representation of a stochastic process. But of course we can convert it into something we can run.\n",
    "\n",
    "Running it means sampling a single path (extending infinitely forward in time) for the dot, and showing that in real-time. This is also simple: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c8607",
   "metadata": {},
   "source": [
    "```haskell\n",
    "gloss :: IO ()\n",
    "gloss = sampleIO $\n",
    "        launchGlossThread defaultSettings\n",
    "            { display = InWindow \"rhine-bayes\" (1024, 960) (10, 10) } \n",
    "        $ reactimateCl glossClock proc () -> do\n",
    "            actualPosition <- prior -< ()\n",
    "            (withSideEffect_ (lift clearIO) >>> visualisation) -< Result { latent = actualPosition }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35359f40",
   "metadata": {},
   "source": [
    "![text](https://monad-bayes-site.netlify.app/images/ezgif-3-1920267e5d.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd234251",
   "metadata": {},
   "source": [
    "## Generative model\n",
    "\n",
    "In standard probabilistic programming, a generative model is a conditional distribution on observations given the latent state, i.e. a function from the latent variable space to distributions over observations.\n",
    "\n",
    "In the real-time setting, a generative model is a function from a process on the latent space to a process on observations. Ours looks like this (pointwise Gaussian noise), but significantly more complex examples are possible: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd0295",
   "metadata": {},
   "source": [
    "```haskell\n",
    "observationModel :: StochasticProcessFunction Position Observation\n",
    "observationModel = proc p -> do\n",
    "    n <- fmap (uncurry V2) $ noise &&& noise -< ()\n",
    "    returnA -< p + n\n",
    "  where noise = constM (normal 0 std)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89216ac",
   "metadata": {},
   "source": [
    "This generates:\n",
    "\n",
    "\n",
    "![text](https://monad-bayes-site.netlify.app/images/measurement.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c7bff",
   "metadata": {},
   "source": [
    "## The posterior \n",
    "\n",
    "The posterior is a function from a process over observations to a process over the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe3f54",
   "metadata": {},
   "source": [
    "```haskell\n",
    "posterior :: StochasticProcessFunction Observation Position\n",
    "posterior = proc (V2 oX oY) -> do\n",
    "  latent@(V2 trueX trueY) <- prior -< ()\n",
    "  observe -< normalPdf oY std trueY * normalPdf oX std trueX\n",
    "  returnA -< latent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5914e",
   "metadata": {},
   "source": [
    "Also note that it is defined in a way that is familiar to probabilistic programming languages: we draw from the prior, and we reweight (this is the factor statement) in a manner that depends on the data. \n",
    "\n",
    "\n",
    "\n",
    "However, the output stochastic process over the position is unnormalized. We need to perform inference to obtain a representation that we can sample from. \n",
    "\n",
    "We do this with Sequential Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81d652",
   "metadata": {},
   "source": [
    "```haskell\n",
    "inferredPosterior :: StochasticProcessFunction Position [(Observation, Weight)]\n",
    "inferredPosterior = particleFilter params {n = 20} posterior\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78443814",
   "metadata": {},
   "source": [
    "Like `posterior`, `inferredPosterior` is a function from one stochastic process to another. \n",
    "\n",
    "However, the output is now a process whose value at a given time is a list of pairs of observations and weights, i.e. a population of particles!\n",
    "\n",
    "This is something we *can* sample from, and the swarm of purple dots is precisely a visualization of the population. So finally the code to produce the original gif:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532a28e",
   "metadata": {},
   "source": [
    "```haskell\n",
    "gloss :: IO ()\n",
    "gloss = sampleIO $\n",
    "        launchGlossThread defaultSettings\n",
    "            { display = InWindow \"rhine-bayes\" (1024, 960) (10, 10) } \n",
    "        $ reactimateCl glossClock proc () -> do\n",
    "            actualPosition <- prior -< ()\n",
    "            measuredPosition <- observationModel -< actualPosition\n",
    "            samples <- particleFilter params posterior -< measuredPosition\n",
    "            (withSideEffect_ (lift clearIO) >>> visualisation) -< Result { \n",
    "                                particles = samples\n",
    "                                , measured = measuredPosition\n",
    "                                , latent = actualPosition\n",
    "                                }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02c80c",
   "metadata": {},
   "source": [
    "This code generates a true trajectory of the particle (`actualPosition`) from the prior. It then uses the `observationModel` to create the true observations. These are passed into model, to obtain samples from the particle filter, which are then passed to a visualizing function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc56aca",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "\n",
    "How does this actually work?\n",
    "\n",
    "It is based on two libraries. First `Monad-Bayes`, which is a probabilistic programming library. Second `Rhine`, which is a functional reactive programming library.\n",
    "\n",
    "Monad-bayes is great at handling *distributions* in a pure and functional manner, but doesn't have a good model of time. However, `Rhine` is superb at handling *time* in a pure and functional way, but don't know about distributions.\n",
    "\n",
    "As such, it turns out that there is a beautiful synergy between the two libraries (or rather, the conceptual approaches to their domains that they represent).\n",
    "\n",
    "More to come...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
